# PacificMeta Game Analysis Dashboard

## ğŸ“„ Table of Contents

- [PacificMeta Game Analysis Dashboard](#pacificmeta-game-analysis-dashboard)
  - [ğŸ“„ Table of Contents](#-table-of-contents)
  - [ğŸš€ Setup Instructions](#-setup-instructions)
  - [ğŸ—ƒï¸ Project Structure](#ï¸-project-structure)
  - [ğŸ§© Core Features](#-core-features)
    - [ğŸ“Š Overall Analytics](#-overall-analytics)
    - [ğŸ® Minigame Analysis](#-minigame-analysis)
    - [ğŸ‘¤ User Analysis](#-user-analysis)
    - [âš™ï¸ Settings Panel](#ï¸-settings-panel)
    - [ğŸ§  AI Integration](#-ai-integration)
  - [ğŸ§¹ Caching System](#-caching-system)
  - [ğŸ§  Using DeepSeek with API](#-using-deepseek-with-api)
  - [ğŸ’» Installing Ollama for Local DeepSeek LLM](#-installing-ollama-for-local-deepseek-llm)

## ğŸš€ Setup Instructions

1. Clone the Repository

    ```bash
    git clone https://github.com/Dogephin/ITP-Projek.git
    ```

2. Install Dependencies

    ```bash
    pip install -r requirements.txt
    ```

3. Configure Environment Variables

    Create a `.env` file in the **root directory** with the following variables:

    ```env
    DEEPSEEK_API_KEY=your-api-key
    DB_HOST=your-database-host
    DB_PORT=3306
    DB_USER=your-db-user
    DB_PASSWORD=your-db-password
    DB_DATABASE=your-db-name
    OLLAMA_PATH=C:/Users/<your-username>/AppData/Local/Programs/Ollama/ollama.exe
    ```

    > â— All environment variables are required. The app will raise an error if any are missing.

4. Run the app

    ```bash
    python app.py
    ```

## ğŸ—ƒï¸ Project Structure

```graphql
â”œâ”€â”€ app.py                      # Main entrypoint
â”œâ”€â”€ config.py                   # Environment configuration
â”œâ”€â”€ analysis/                   # Logic for AI analysis
â”‚   â”œâ”€â”€ minigames_analysis.py
â”‚   â”œâ”€â”€ overall_analysis.py
â”‚   â””â”€â”€ user_analysis.py
â”œâ”€â”€ routes/                     # Flask routes
â”‚   â”œâ”€â”€ home.py
â”‚   â”œâ”€â”€ minigame.py
â”‚   â”œâ”€â”€ overall.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ user.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ db.py                   # Database initialisation
â”‚   â”œâ”€â”€ cache.py                # Cache management, cache key generation
â”‚   â”œâ”€â”€ context.py              # Helper function for retrieving LLM client
â”‚   â””â”€â”€ llm.py                  # LLM initialisation
â”œâ”€â”€ templates/                  # HTML Jinja templates
â”œâ”€â”€ static/                     # CSS, JS, assets
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ .env                        # Environment variables
```

## ğŸ§© Core Features

### ğŸ“Š Overall Analytics

- Average vs Max scores per minigame
- Error frequency heatmap over time
- Performance vs duration scatter plot
- AI summary of user training behavior

### ğŸ® Minigame Analysis

- Summary statistics: attempts, scores, completion rates
- Top minor and severe errors
- AI-generated executive summaries per game
- Search & Sort minigames by name or average score for quick lookup
- Filtering Options (**Total Attempts**, **Completion Rate**, **Average Score**, **Failed**) for easier navigation.

### ğŸ‘¤ User Analysis

- Game-specific and overall assessment
- Trend analysis across attempts
- Single attempt AI analysis
- Bulk AI evaluation with strengths/weakness insights

### âš™ï¸ Settings Panel

- Toggle between `API` and `LOCAL` query type
- One-click cache clearing

### ğŸ§  AI Integration

- Switch between DeepSeek API or local LLMs via Ollama
- Caching of AI responses to avoid repeated API calls for the same query
- Regeneration of AI responses from UI
- Download AI responses to text file for future analysis

## ğŸ§¹ Caching System

- AI responses are cached to `llm_cache/`

- Caching configuration in `app.config`:

    ```python
    CACHE_TYPE = "FileSystemCache" # Cache locally
    CACHE_DIR = "llm_cache" # Directory where cache records are stored
    CACHE_DEFAULT_TIMEOUT = 3600  # Cache expiry in seconds
    CACHE_THRESHOLD = 20 # Max number of cache records
    ```

  - In the code above, cached LLM responses auto-expire after **1 hour** or when **more than 20 entries** exist.

## ğŸ§  Using DeepSeek with API

If using API-based LLM analysis:

- Set `DEEPSEEK_API_KEY` in `.env`
- Start app and select `API` in [settings](http://127.0.0.1:5000/settings)

## ğŸ’» Installing Ollama for Local DeepSeek LLM

1. **Download and Install Ollama**

    - Download Ollama from: <https://ollama.com/download>

2. **Configure the Path to `ollama.exe`**

    - Add the path to the `ollama.exe` executable in your `.env` file. For example:

        ```env
        OLLAMA_PATH=C:/Users/<your-username>/AppData/Local/Programs/Ollama/ollama.exe
        ```

3. **Install a DeepSeek Model**

    - Browse and choose a DeepSeek model from the library: <https://ollama.com/library/deepseek-r1>

    - Open a terminal and run the following command to download the model:

        ```bash
        ollama pull deepseek-r1:14b
        ```

        > **Change the model name to the DeepSeek model you want**

4. **Configure the Application**

    - Launch the app and navigate to the [settings page](http://127.0.0.1:5000/settings).

    - Set the AI type to `LOCAL` and select the name of the installed model from the dropdown.

---
