# PacificMeta Game Analysis Dashboard

## ğŸ“„ Table of Contents

- [PacificMeta Game Analysis Dashboard](#pacificmeta-game-analysis-dashboard)
  - [ğŸ“„ Table of Contents](#-table-of-contents)
  - [ğŸš€ Setup Instructions](#-setup-instructions)
    - [ğŸ’» Method 1: Running the App diectly via Python](#-method-1-running-the-app-diectly-via-python)
    - [ğŸ³ Method 2: Docker Compose Setup](#-method-2-docker-compose-setup)
      - [ğŸ”„ Load Balancing Test](#-load-balancing-test)
    - [ğŸ”’ Authentication and User Roles](#-authentication-and-user-roles)
      - [ğŸ”‘ Login and Security](#-login-and-security)
      - [ğŸ‘©â€ğŸ« User Roles and Data Access](#-user-roles-and-data-access)
  - [ğŸ—ƒï¸ Project Structure](#ï¸-project-structure)
  - [ğŸ§© Core Features](#-core-features)
    - [ğŸ“Š Overall Analytics](#-overall-analytics)
    - [ğŸ® Minigame Analysis](#-minigame-analysis)
    - [ğŸ‘¤ User Analysis](#-user-analysis)
    - [âš™ï¸ Settings Panel](#ï¸-settings-panel)
    - [ğŸ§  AI Integration](#-ai-integration)
  - [ğŸ§¹ Caching System](#-caching-system)
  - [ğŸ§  Using DeepSeek with API](#-using-deepseek-with-api)
  - [ğŸ’» Installing Ollama for Local DeepSeek LLM](#-installing-ollama-for-local-deepseek-llm)

## ğŸš€ Setup Instructions

> [!IMPORTANT]
> There are currently two ways to run the application. The first way is to directly run the app using Python. The second way is to run the application using Docker

### ğŸ’» Method 1: Running the App diectly via Python

1. Clone the Repository

    ```bash
    git clone https://github.com/Dogephin/AI_Dashboard.git
    ```

2. Install Dependencies

    ```bash
    pip install -r requirements.txt
    ```

3. Configure Environment Variables

    Create a `.env` file in the **root directory** with the following variables:

    ```env
    DEEPSEEK_API_KEY=your-api-key
    DB_HOST=your-database-host
    DB_PORT=3306
    DB_USER=your-db-user
    DB_PASSWORD=your-db-password
    DB_DATABASE=your-db-name
    OLLAMA_PATH=C:/Users/<your-username>/AppData/Local/Programs/Ollama/ollama.exe
    ```

    > â— All environment variables are required. The app will raise an error if any are missing.

4. Run the app

    ```bash
    python app.py
    ```

### ğŸ³ Method 2: Docker Compose Setup

> [!TIP]
> This is the recommended way to run the application, as it includes the **Nginx reverse proxy** and **load balancer** and runs the Flask application in a container.

1. Install [Docker Desktop](https://www.docker.com/get-started/) on your computer.

2. Configure Environment Variables
     - Ensure your `.env` file (as described in the previous method) is in the root directory.

3. Build and Run the Containers

    - Ensure that the Docker Engine is up and running in your computer before proceeding.

    - This command below builds the web service (Flask app) and starts all services in detached mode.

        ```bash
        docker-compose up --build -d
        ```

4. Access the Dashboard

    - The dashboard will be available at: <http://localhost:80> (or your host's IP address) because Nginx listens on port 80 and proxies requests to the Flask backend.

#### ğŸ”„ Load Balancing Test

- The setup uses **Nginx** as a reverse proxy to load balance requests across potentially multiple Flask application instances (containers).

- To test if the round-robin load balancing is working, you can scale the web service and hit the `/whoami` endpoint multiple times:

    ```bash
    docker-compose up --scale web=3 -d
    # Then access http://localhost/whoami repeatedly in your browser
    ```

- The response for the `/whoami` route will show a different container hostname each time, confirming that Nginx is distributing the requests.

### ğŸ”’ Authentication and User Roles

- The dashboard uses a session-based authentication system to manage access and tailor the data displayed.

#### ğŸ”‘ Login and Security

- Upon launching the application, you will be redirected to the [login page](http://localhost/login).
- All dashboard routes are protected by the `@login_required` decorator.
- User credentials are checked against the `AdminAccount` table using an MD5 hash comparison for the password.

#### ğŸ‘©â€ğŸ« User Roles and Data Access

Upon successful login, a session role is assigned based on the username:

| **Role**          | **Username**                                  | **Data Access Scope**                                                   |
|-------------------|-----------------------------------------------|-------------------------------------------------------------------------|
| Admin (`admin`)     | Default (Username does not contain `"teacher"`) | Sees all student data across the application.                           |
| Teacher (`teacher`) | Username contains `"teacher"`                   | Only sees data for students linked to their account via `IMA_Admin_User`. |

## ğŸ—ƒï¸ Project Structure

```graphql
â”œâ”€â”€ app.py                      # Main entrypoint
â”œâ”€â”€ config.py                   # Environment configuration
â”œâ”€â”€ Dockerfile Â  Â  Â  Â  Â  Â  Â  Â  Â # Docker image definition for the Flask app 
â”œâ”€â”€ docker-compose.yml Â  Â  Â  Â  Â # Defines web (Flask) and nginx services
â”œâ”€â”€ nginx.conf Â  Â  Â  Â  Â  Â  Â  Â  Â # Nginx reverse proxy and load balancer configuration
â”œâ”€â”€ analysis/                   # Logic for AI analysis
â”‚   â”œâ”€â”€ minigames_analysis.py
â”‚   â”œâ”€â”€ overall_analysis.py
â”‚   â””â”€â”€ user_analysis.py
â”œâ”€â”€ routes/                     # Flask routes
â”‚   â”œâ”€â”€ home.py
â”‚ Â  â”œâ”€â”€ login.py Â  Â  Â  Â  Â  Â  Â  Â # User login/logout routes
â”‚   â”œâ”€â”€ minigame.py
â”‚   â”œâ”€â”€ overall.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ user.py
â”œâ”€â”€ utils/
â”‚ Â  â”œâ”€â”€ auth.py Â  Â  Â  Â  Â  Â  Â  Â  # Login required decorator for authentication
â”‚   â”œâ”€â”€ db.py                   # Database initialisation
â”‚   â”œâ”€â”€ cache.py                # Cache management, cache key generation
â”‚   â”œâ”€â”€ context.py              # Helper function for retrieving LLM client
â”‚   â””â”€â”€ llm.py                  # LLM initialisation
â”œâ”€â”€ templates/                  # HTML Jinja templates
â”œâ”€â”€ static/                     # CSS, JS, assets
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ .env                        # Environment variables
```

## ğŸ§© Core Features

### ğŸ“Š Overall Analytics

- Average vs Max scores per minigame
- Error frequency heatmap over time
- Performance vs duration scatter plot
- Monthly average score trend line for all minigames
- Error vs Completion Time scatter plot
- Top 3 vs Bottom 3 student performance analysis
- Personalized student feedback based on performance
- AI summary of overall user training behaviour

### ğŸ® Minigame Analysis

- Summary statistics: attempts, scores, completion rates
- Top minor and severe errors
- Monthly warning trend analysis (Current month vs. Last month)
- AI-generated executive summaries per game
- AI-powered prioritization brief for low-performing minigames
- Search & Sort minigames by name or average score for quick lookup
- Filtering Options (**Total Attempts**, **Completion Rate**, **Average Score**, **Failed**) for easier navigation.

### ğŸ‘¤ User Analysis

- Game-specific and overall assessment
- Trend analysis across attempts
- Single attempt AI analysis
- Bulk AI evaluation with strengths/weakness insights
- AI categorization and recommendation for all time errors
- User grouping based on ID (e.g., 'Staff', 'Year XXXX Cohort').

### âš™ï¸ Settings Panel

- Toggle between `API` and `LOCAL` query type
- One-click cache clearing

### ğŸ§  AI Integration

- Switch between DeepSeek API or local LLMs via Ollama
- Caching of AI responses to avoid repeated API calls for the same query
- Regeneration of AI responses from UI
- Download AI responses to text file for future analysis

## ğŸ§¹ Caching System

- AI responses are cached to `llm_cache/`

- Caching configuration in `app.config`:

    ```python
    CACHE_TYPE = "FileSystemCache" # Cache locally
    CACHE_DIR = "llm_cache" # Directory where cache records are stored
    CACHE_DEFAULT_TIMEOUT = 3600  # Cache expiry in seconds
    CACHE_THRESHOLD = 20 # Max number of cache records
    ```

  - In the code above, cached LLM responses auto-expire after **1 hour** or when **more than 20 entries** exist.

## ğŸ§  Using DeepSeek with API

If using API-based LLM analysis:

- Set `DEEPSEEK_API_KEY` in `.env`
- Start app and select `API` in [settings](http://127.0.0.1:5000/settings)

## ğŸ’» Installing Ollama for Local DeepSeek LLM

1. **Download and Install Ollama**

    - Download Ollama from: <https://ollama.com/download>

2. **Configure the Path to `ollama.exe`**

    - Add the path to the `ollama.exe` executable in your `.env` file. For example:

        ```env
        OLLAMA_PATH=C:/Users/<your-username>/AppData/Local/Programs/Ollama/ollama.exe
        ```

3. **Install a DeepSeek Model**

    - Browse and choose a DeepSeek model from the library: <https://ollama.com/library/deepseek-r1>

    - Open a terminal and run the following command to download the model:

        ```bash
        ollama pull deepseek-r1:14b
        ```

        > **Change the model name to the DeepSeek model you want**

4. **Configure the Application**

    - Launch the app and navigate to the [settings page](http://127.0.0.1:5000/settings).

    - Set the AI type to `LOCAL` and select the name of the installed model from the dropdown.

---
